"""
arXivè«–æ–‡æŠ½å‡ºãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’çµ±åˆã—ãŸå®Ÿè¡Œå¯èƒ½ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""

import argparse
import sys
import time
import yaml
from pathlib import Path
from typing import Dict, List, Optional, Union
from loguru import logger

# å†…éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from .arxiv_api import create_arxiv_client, ArxivPaper
from .ocr_processor import create_ocr_manager
from .reference_cleaner import create_reference_cleaner
from .text_processor import create_text_processor
from .llm_extractor import create_llm_extractor
from .csv_generator import create_csv_generator


class ArxivExtractor:
    """arXivè«–æ–‡æŠ½å‡ºãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(self, config_path: str = "config/config.yaml"):
        """
        åˆæœŸåŒ–
        
        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        self.config = self._load_config(config_path)
        self.setup_logging()
        
        # å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–
        self.arxiv_client = create_arxiv_client(self.config)
        self.ocr_manager = create_ocr_manager(self.config)
        self.reference_cleaner = create_reference_cleaner(self.config)
        self.text_processor = create_text_processor(self.config)
        self.llm_extractor = create_llm_extractor(self.config)
        self.csv_generator = create_csv_generator(self.config)
        
        logger.info("ArxivExtractor initialized successfully")
    
    def _load_config(self, config_path: str) -> Dict:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"Configuration loaded from: {config_path}")
            return config
        except FileNotFoundError:
            logger.warning(f"Config file not found: {config_path}, using defaults")
            return self._get_default_config()
        except Exception as e:
            logger.error(f"Failed to load config: {str(e)}")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’è¿”ã™"""
        return {
            'arxiv': {
                'base_url': 'http://export.arxiv.org/api/query',
                'rate_limit': {'requests_per_second': 1},
                'pdf': {'timeout': 30, 'max_retries': 3}
            },
            'ocr': {
                'model': 'unstructured',
                'fallback_models': ['surya'],
                'device': 'cpu'
            },
            'text_processing': {
                'remove_references': True,
                'citation_cleanup': True,
                'min_text_length': 1000
            },
            'llm': {
                'model': 'gpt-4',
                'fallback_models': ['gpt-3.5-turbo'],
                'openai': {'api_key': None}
            },
            'output': {
                'format': 'csv',
                'output_dir': './output'
            },
            'logging': {
                'level': 'INFO'
            }
        }
    
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®šã‚’åˆæœŸåŒ–"""
        log_config = self.config.get('logging', {})
        level = log_config.get('level', 'INFO')
        
        # æ—¢å­˜ã®ãƒ­ã‚¬ãƒ¼ã‚’å‰Šé™¤
        logger.remove()
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›è¨­å®š
        if log_config.get('console', {}).get('enabled', True):
            console_format = log_config.get('console', {}).get(
                'format', 
                "{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}"
            )
            logger.add(sys.stderr, level=level, format=console_format)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›è¨­å®š
        if log_config.get('file', {}).get('enabled', False):
            file_path = log_config.get('file', {}).get('path', './logs/arxiv_scraper.log')
            Path(file_path).parent.mkdir(parents=True, exist_ok=True)
            
            logger.add(
                file_path,
                level=level,
                rotation=log_config.get('file', {}).get('max_size', '10MB'),
                retention=log_config.get('file', {}).get('backup_count', 5)
            )
    
    def process_single_paper(
        self, 
        identifier: str, 
        output_filename: Optional[str] = None
    ) -> Dict:
        """
        å˜ä¸€è«–æ–‡ã‚’å‡¦ç†
        
        Args:
            identifier: arXiv IDã€URLã€ã¾ãŸã¯PDFãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            output_filename: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            å‡¦ç†çµæœè¾æ›¸
        """
        logger.info(f"Starting processing for: {identifier}")
        start_time = time.time()
        
        try:
            # Step 1: è«–æ–‡æƒ…å ±å–å¾—
            if identifier.endswith('.pdf'):
                # PDFãƒ•ã‚¡ã‚¤ãƒ«ç›´æ¥æŒ‡å®šã®å ´åˆ
                pdf_path = identifier
                paper_metadata = None
            else:
                # arXiv ID/URLã®å ´åˆ
                paper = self.arxiv_client.get_paper_metadata(identifier)
                if not paper:
                    return {
                        'success': False,
                        'error': f'è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {identifier}',
                        'processing_time': time.time() - start_time
                    }
                
                # PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                pdf_path = self.arxiv_client.download_pdf(paper)
                if not pdf_path:
                    return {
                        'success': False,
                        'error': 'PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ',
                        'processing_time': time.time() - start_time
                    }
                
                paper_metadata = {
                    'title': paper.title,
                    'authors': paper.authors,
                    'categories': paper.categories,
                    'published': paper.published,
                    'abs_url': paper.abs_url
                }
            
            # Step 2: OCRå‡¦ç†
            logger.info("Starting OCR processing...")
            ocr_start = time.time()
            ocr_result = self.ocr_manager.process_pdf(pdf_path)
            ocr_time = time.time() - ocr_start
            
            if not ocr_result.text:
                return {
                    'success': False,
                    'error': 'OCRå‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ',
                    'processing_time': time.time() - start_time
                }
            
            logger.info(f"OCR completed in {ocr_time:.2f}s using {ocr_result.model_used}")
            
            # Step 3: Referenceå‰Šé™¤
            logger.info("Starting reference removal...")
            ref_start = time.time()
            cleaned_result = self.reference_cleaner.clean_text(ocr_result.text)
            ref_time = time.time() - ref_start
            
            logger.info(f"Reference removal completed in {ref_time:.2f}s (confidence: {cleaned_result.confidence:.3f})")
            
            # Step 4: ãƒ†ã‚­ã‚¹ãƒˆå¾Œå‡¦ç†
            logger.info("Starting text post-processing...")
            text_start = time.time()
            processed_result = self.text_processor.process_text(cleaned_result.cleaned_text)
            text_time = time.time() - text_start
            
            logger.info(f"Text processing completed in {text_time:.2f}s (quality: {processed_result.quality_score:.3f})")
            
            # Step 5: LLMæŠ½å‡º
            logger.info("Starting LLM extraction...")
            llm_start = time.time()
            extraction_result = self.llm_extractor.extract_from_text(
                processed_result.text, 
                paper_metadata
            )
            llm_time = time.time() - llm_start
            
            if not extraction_result.success:
                logger.warning("LLM extraction had issues, but proceeding with available data")
            
            logger.info(f"LLM extraction completed in {llm_time:.2f}s using {extraction_result.model_used}")
            
            # Step 6: CSVå‡ºåŠ›
            total_processing_time = time.time() - start_time
            csv_path = self.csv_generator.write_single_record(
                extraction_result,
                output_filename,
                total_processing_time
            )
            
            logger.info(f"Processing completed successfully in {total_processing_time:.2f}s")
            logger.info(f"Output saved to: {csv_path}")
            
            return {
                'success': True,
                'csv_path': csv_path,
                'processing_time': total_processing_time,
                'stages': {
                    'ocr_time': ocr_time,
                    'reference_time': ref_time,
                    'text_time': text_time,
                    'llm_time': llm_time
                },
                'quality_metrics': {
                    'ocr_confidence': ocr_result.confidence,
                    'reference_confidence': cleaned_result.confidence,
                    'text_quality': processed_result.quality_score,
                    'llm_confidence': sum(extraction_result.confidence_scores.values()) / len(extraction_result.confidence_scores) if extraction_result.confidence_scores else 0.0
                },
                'extraction_data': extraction_result.data
            }
            
        except Exception as e:
            logger.error(f"Processing failed: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'processing_time': time.time() - start_time
            }
    
    def process_batch(
        self, 
        identifiers: List[str], 
        output_filename: Optional[str] = None
    ) -> Dict:
        """
        è¤‡æ•°è«–æ–‡ã‚’ãƒãƒƒãƒå‡¦ç†
        
        Args:
            identifiers: arXiv IDã€URLã€ã¾ãŸã¯PDFãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
            output_filename: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            ãƒãƒƒãƒå‡¦ç†çµæœè¾æ›¸
        """
        logger.info(f"Starting batch processing for {len(identifiers)} papers")
        start_time = time.time()
        
        results = []
        extraction_results = []
        processing_times = []
        
        for i, identifier in enumerate(identifiers, 1):
            logger.info(f"Processing paper {i}/{len(identifiers)}: {identifier}")
            
            result = self.process_single_paper(identifier)
            results.append(result)
            
            if result['success']:
                # æˆåŠŸã—ãŸå ´åˆã€CSVã«ã¯æ›¸ãè¾¼ã¾ãšã«çµæœã‚’è“„ç©
                extraction_results.append(result['extraction_data'])
                processing_times.append(result['processing_time'])
            else:
                logger.error(f"Failed to process {identifier}: {result.get('error', 'Unknown error')}")
                # å¤±æ•—ã—ãŸå ´åˆã¯N/Aãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’è¿½åŠ 
                from .llm_extractor import ExtractionResult
                failed_result = ExtractionResult(
                    success=False,
                    data={
                        "æ‰‹æ³•ã®è‚": "N/A",
                        "åˆ¶é™äº‹é …": "N/A", 
                        "å¯¾è±¡ãƒŠãƒ¬ãƒƒã‚¸": "N/A",
                        "URL": identifier if not identifier.endswith('.pdf') else "N/A",
                        "ã‚¿ã‚¤ãƒˆãƒ«": "N/A",
                        "å‡ºç‰ˆå¹´": "N/A",
                        "ç ”ç©¶åˆ†é‡": "N/A",
                        "èª²é¡Œè¨­å®š": "N/A",
                        "è«–æ–‡ã®ä¸»å¼µ": "N/A"
                    },
                    model_used="failed",
                    processing_time=result['processing_time'],
                    confidence_scores={},
                    errors=[result.get('error', 'Processing failed')],
                    metadata={}
                )
                extraction_results.append(failed_result)
                processing_times.append(result['processing_time'])
        
        # ãƒãƒƒãƒCSVå‡ºåŠ›
        if extraction_results:
            csv_path = self.csv_generator.write_batch_records(
                extraction_results,
                output_filename,
                processing_times
            )
        else:
            csv_path = None
        
        total_time = time.time() - start_time
        success_count = sum(1 for r in results if r['success'])
        
        logger.info(f"Batch processing completed: {success_count}/{len(identifiers)} successful in {total_time:.2f}s")
        if csv_path:
            logger.info(f"Batch output saved to: {csv_path}")
        
        return {
            'success': success_count > 0,
            'csv_path': csv_path,
            'total_processing_time': total_time,
            'success_count': success_count,
            'total_count': len(identifiers),
            'individual_results': results
        }
    
    def process_from_file(
        self, 
        input_file: str, 
        output_filename: Optional[str] = None
    ) -> Dict:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è«–æ–‡ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚“ã§ãƒãƒƒãƒå‡¦ç†
        
        Args:
            input_file: è«–æ–‡IDãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ1è¡Œ1IDï¼‰
            output_filename: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            å‡¦ç†çµæœè¾æ›¸
        """
        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                identifiers = [line.strip() for line in f if line.strip()]
            
            logger.info(f"Loaded {len(identifiers)} identifiers from {input_file}")
            return self.process_batch(identifiers, output_filename)
            
        except FileNotFoundError:
            logger.error(f"Input file not found: {input_file}")
            return {
                'success': False,
                'error': f'å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_file}'
            }
        except Exception as e:
            logger.error(f"Failed to process file: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }


def main():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å®Ÿè¡Œç”¨ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description="arXivè«–æ–‡ã‹ã‚‰9é …ç›®ã‚’è‡ªå‹•æŠ½å‡ºã—CSVå‡ºåŠ›",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # å˜ä¸€è«–æ–‡å‡¦ç†
  python -m src.arxiv_extractor --id 2301.12345
  python -m src.arxiv_extractor --url https://arxiv.org/abs/2301.12345
  python -m src.arxiv_extractor --pdf paper.pdf
  
  # ãƒãƒƒãƒå‡¦ç†
  python -m src.arxiv_extractor --batch paper_list.txt
  python -m src.arxiv_extractor --ids 2301.12345,2301.12346,2301.12347
  
  # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«æŒ‡å®š
  python -m src.arxiv_extractor --id 2301.12345 --config config/custom_config.yaml
        """
    )
    
    # å…¥åŠ›ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆæ’ä»–çš„ï¼‰
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument('--id', help='arXiv ID (ä¾‹: 2301.12345)')
    input_group.add_argument('--url', help='arXiv URL (ä¾‹: https://arxiv.org/abs/2301.12345)')
    input_group.add_argument('--pdf', help='PDFãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    input_group.add_argument('--batch', help='è«–æ–‡IDãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ1è¡Œ1IDï¼‰')
    input_group.add_argument('--ids', help='ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®è«–æ–‡IDãƒªã‚¹ãƒˆ')
    
    # ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument('--config', '-c', default='config/config.yaml', 
                       help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: config/config.yamlï¼‰')
    parser.add_argument('--output', '-o', help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å')
    parser.add_argument('--verbose', '-v', action='store_true', help='è©³ç´°ãƒ­ã‚°å‡ºåŠ›')
    parser.add_argument('--quiet', '-q', action='store_true', help='ã‚¨ãƒ©ãƒ¼ã®ã¿å‡ºåŠ›')
    
    args = parser.parse_args()
    
    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«èª¿æ•´
    if args.verbose:
        logger.remove()
        logger.add(sys.stderr, level="DEBUG")
    elif args.quiet:
        logger.remove()
        logger.add(sys.stderr, level="ERROR")
    
    try:
        # Extractorã‚’åˆæœŸåŒ–
        extractor = ArxivExtractor(args.config)
        
        # å…¥åŠ›ã«å¿œã˜ã¦å‡¦ç†å®Ÿè¡Œ
        if args.id:
            result = extractor.process_single_paper(args.id, args.output)
        elif args.url:
            result = extractor.process_single_paper(args.url, args.output)
        elif args.pdf:
            result = extractor.process_single_paper(args.pdf, args.output)
        elif args.batch:
            result = extractor.process_from_file(args.batch, args.output)
        elif args.ids:
            identifiers = [id.strip() for id in args.ids.split(',') if id.strip()]
            result = extractor.process_batch(identifiers, args.output)
        
        # çµæœå‡ºåŠ›
        if result['success']:
            print(f"âœ… å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
            if 'csv_path' in result and result['csv_path']:
                print(f"ğŸ“„ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {result['csv_path']}")
            
            if 'success_count' in result:
                print(f"ğŸ“Š æˆåŠŸç‡: {result['success_count']}/{result['total_count']}")
            
            processing_time = result.get('processing_time', result.get('total_processing_time', 0))
            print(f"â±ï¸  å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
            
            sys.exit(0)
        else:
            print(f"âŒ å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {result.get('error', 'Unknown error')}")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nâš ï¸  å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()