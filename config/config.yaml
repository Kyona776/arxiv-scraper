# arXiv Scraper Configuration

# OCR Processing Settings
ocr:
  # Primary OCR model to use
  model: "mistral_ocr"  # Options: mistral_ocr, nougat, unstructured, surya
  
  # Device settings
  device: "cuda"  # Options: cuda, cpu, mps
  
  # Batch processing
  batch_size: 4
  
  # Fallback models in order of preference
  fallback_models:
    - "nougat"
    - "unstructured"
    - "surya"
  
  # OCR specific settings
  mistral_ocr:
    max_length: 2048
    temperature: 0.1
    
  # Mistral OCR API settings (direct API)
  mistral_ocr_api:
    api_key: null  # Set via environment variable MISTRAL_API_KEY
    base_url: "https://api.mistral.ai"
    model: "mistral-ocr-latest"
    include_image_base64: true
    max_retries: 3
    retry_delay: 1.0
    timeout: 120
    
  # Mistral OCR OpenRouter API settings
  mistral_ocr_openrouter:
    api_key: null  # Set via environment variable OPENROUTER_API_KEY
    base_url: "https://openrouter.ai/api/v1"
    model: "mistral/mistral-ocr-latest"
    site_url: "https://arxiv-scraper.local"
    site_name: "ArXiv Scraper"
    max_retries: 3
    retry_delay: 1.0
    timeout: 120
  
  nougat:
    model_size: "base"  # Options: base, small
    recompute: false
  
  unstructured:
    strategy: "hi_res"  # Options: hi_res, fast, ocr_only
    model_name: "yolox"
  
  surya:
    batch_size: 2
    det_batch_size: 2

# Text Processing Settings
text_processing:
  # Reference removal settings
  remove_references: true
  
  # Reference section detection patterns
  reference_patterns:
    - "REFERENCES"
    - "References"
    - "BIBLIOGRAPHY"
    - "Bibliography"
    - "参考文献"
    - "文献"
    - "Literature Cited"
    - "LITERATURE CITED"
  
  # Citation patterns to clean from main text
  citation_patterns:
    - r"\[(\d+(?:[-–,]\s*\d+)*)\]"  # [1], [1-3], [1,2,3]
    - r"\(([A-Za-z]+\s+(?:et\s+al\.?\s*)?(?:19|20)\d{2}[a-z]?(?:;\s*[A-Za-z]+\s+(?:et\s+al\.?\s*)?(?:19|20)\d{2}[a-z]?)*)\)"  # (Author 2023)
    - r"\((?:19|20)\d{2}[a-z]?\)"  # (2023)
  
  # Text cleanup settings
  citation_cleanup: true
  remove_page_numbers: true
  remove_headers_footers: true
  normalize_whitespace: true
  
  # Quality thresholds
  min_text_length: 1000  # Minimum text length after processing
  max_reference_ratio: 0.3  # Maximum ratio of references to total text

# LLM Processing Settings
llm:
  # Primary LLM model
  model: "anthropic/claude-4-sonnet"  # Options: gpt-4, gpt-3.5-turbo, claude-3-opus, claude-3-sonnet, or OpenRouter models
  
  # Generation parameters
  temperature: 0.1
  max_tokens: 2000
  top_p: 1.0
  frequency_penalty: 0.0
  presence_penalty: 0.0
  
  # Retry settings
  max_retries: 3
  retry_delay: 1.0
  
  # Fallback models
  fallback_models:
    - "anthropic/claude-3-haiku"
    - "openai/gpt-4"
    - "mistral/mistral-large"
    - "gpt-3.5-turbo"
    - "claude-3-sonnet"
  
  # API settings
  openai:
    api_key: null  # Set via environment variable OPENAI_API_KEY
    organization: null
    
  anthropic:
    api_key: null  # Set via environment variable ANTHROPIC_API_KEY
  
  # OpenRouter API settings
  openrouter:
    api_key: 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  # Set via environment variable OPENROUTER_API_KEY
    base_url: "https://openrouter.ai/api/v1"
    model: "moonshotai/kimi-k2"  # Default model
    reasoning_model: # Default reasoning model
      name: "moonshotai/kimi-k2"
      provider: "anthropic"  # Default provider
    site_url: "https://arxiv-scraper.local"
    site_name: "ArXiv Scraper"
    temperature: 0.1
    max_tokens: 2000
    max_retries: 3
    retry_delay: 1.0
    timeout: 120
    
    # Caching settings
    cache_enabled: true
    cache_ttl: 3600  # 1 hour
    cache_dir: "./cache/openrouter"
    
    # Model selection preferences
    model_preferences:
      task_types:
        general: ["anthropic/claude-3-sonnet", "openai/gpt-4", "google/gemini-pro"]
        coding: ["anthropic/claude-3-opus", "openai/gpt-4", "google/gemini-pro"]
        conversation: ["anthropic/claude-3-sonnet", "anthropic/claude-3-haiku", "openai/gpt-3.5-turbo"]
        analysis: ["anthropic/claude-3-opus", "openai/gpt-4", "google/gemini-pro"]
      
      # Budget constraints (cost per token)
      budget_tiers:
        low: 0.000001  # Under $0.000001 per token
        medium: 0.00001  # Under $0.00001 per token
        high: 0.00005   # Under $0.00005 per token
        unlimited: null
    
    # Provider filtering
    provider_preferences:
      # Preferred providers (in order)
      preferred: ["anthropic", "openai", "google", "mistral", "baseten/fp8"]
      
      # Providers to avoid
      blocked: []
      
      # Privacy requirements
      privacy_requirements:
        no_logging: true  # Avoid providers that may log prompts
        no_training: true  # Avoid providers that may train on data
        moderated_only: false  # Only use moderated providers
    
    # Model filtering
    model_filters:
      min_context_length: 8000  # Minimum context length
      max_cost_per_token: 0.00005  # Maximum cost per token
      required_modalities: ["text->text"]  # Required input/output modalities
      
      # Exclude specific models
      exclude_models: []
    
    # Endpoint selection criteria
    endpoint_preferences:
      selection_criteria: "cost"  # Options: cost, latency, uptime, throughput
      min_uptime: 0.95  # Minimum uptime percentage
      max_latency: 5000  # Maximum latency in milliseconds
      
      # Fallback strategy
      fallback_strategy: "next_available"  # Options: next_available, best_alternative, fail
    
    # Popular OpenRouter models (for reference)
    available_models:
      - "moonshotai/kimi-k2"

# Extraction Target Items
extraction_items:
  - name: "手法の肝"
    description: "論文の核となる技術手法・アプローチ"
    max_length: 500
    
  - name: "制限事項"
    description: "手法の限界や制約条件"
    max_length: 500
    
  - name: "対象ナレッジ"
    description: "扱う知識領域・データ種別"
    max_length: 500
    
  - name: "URL"
    description: "arXiv論文のURL"
    max_length: 200
    
  - name: "タイトル"
    description: "論文タイトル"
    max_length: 300
    
  - name: "出版年"
    description: "発表年"
    max_length: 50
    
  - name: "研究分野"
    description: "分野分類"
    max_length: 200
    
  - name: "課題設定"
    description: "解決しようとする問題"
    max_length: 500
    
  - name: "論文の主張"
    description: "主要な貢献・結論"
    max_length: 500

# Output Settings
output:
  # File format
  format: "csv"  # Options: csv, json, xlsx
  
  # CSV settings
  csv:
    encoding: "utf-8"
    delimiter: ","
    quotechar: '"'
    quoting: "minimal"  # Options: minimal, all, non_numeric, none
    
  # JSON settings  
  json:
    indent: 2
    ensure_ascii: false
    
  # Default values for missing data
  default_values:
    missing_data: "N/A"
    error_data: "ERROR"
    
  # Output directory
  output_dir: "./output"
  
  # Filename patterns
  filename_patterns:
    single: "arxiv_extraction_{timestamp}.csv"
    batch: "arxiv_batch_{timestamp}.csv"

# Logging Settings
logging:
  level: "INFO"  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
  
  # Log file settings
  file:
    enabled: true
    path: "./logs/arxiv_scraper.log"
    max_size: "10MB"
    backup_count: 5
    
  # Console logging
  console:
    enabled: true
    format: "{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}"
    
  # Component-specific logging
  components:
    ocr: "INFO"
    reference_cleaner: "DEBUG"
    llm_extractor: "INFO"
    arxiv_api: "INFO"

# Performance Settings
performance:
  # Processing timeouts (seconds)
  timeouts:
    pdf_download: 30
    ocr_processing: 300
    reference_removal: 10
    llm_extraction: 120
    
  # Memory settings
  memory:
    max_pdf_size: "50MB"
    max_text_length: 100000
    
  # Batch processing
  batch:
    max_concurrent: 3
    chunk_size: 10
    
  # Caching
  cache:
    enabled: true
    ttl: 3600  # seconds
    max_size: 100

# arXiv API Settings
arxiv:
  # API endpoint
  base_url: "http://export.arxiv.org/api/query"
  
  # Request settings
  max_results: 1
  sort_by: "relevance"
  sort_order: "descending"
  
  # Rate limiting
  rate_limit:
    requests_per_second: 1
    burst_size: 3
    
  # PDF download settings
  pdf:
    user_agent: "arXiv-Scraper/1.0"
    timeout: 30
    max_retries: 3

# ------------------------------------------------------------------------------
# Research Assistant Settings
# ------------------------------------------------------------------------------
research_assistant:
  embedding_batch_size: 128
  project_base_path: "output/research_projects"
  default_model: # Default reasoning model
      name: "moonshotai/kimi-k2"
      provider:
        order: ["baseten/fp8","moonshotai"]

  reasoning_model: # Default reasoning model
      name: "moonshotai/kimi-k2"
      provider:
        order: ["baseten/fp8","moonshotai"]

  